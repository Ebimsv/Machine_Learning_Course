{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Linear Regression with Stochastic Gradient Descent from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates implementing univariate linear regression using Stochastic Gradient Descent (SGD) from scratch. We'll use Scikit-Learn's data generator, compute gradients manually, and iteratively optimize the model parameters. We'll explain each step in detail, focusing on the gradient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Gradient Descent\n",
    "Gradient Descent is an optimization algorithm to minimize a loss function by iteratively updating the model parameters (weight ùë§ and bias b).\n",
    "- **Loss Function**: Mean Squared Error (MSE):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mse loss](https://raw.githubusercontent.com/Ebimsv/Machine_Learning_Course/refs/heads/main/pics/MSE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, ùë•_ùëñ is the feature value, y_i is the true value, and w, b are the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient Computation**: The gradients of the loss function w.r.t ùë§ and b are derived as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient computation](https://raw.githubusercontent.com/Ebimsv/Machine_Learning_Course/refs/heads/main/pics/Gradient_computation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stochastic Gradient Descent, we compute these gradients for a single data point at a time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gradients_w_b](https://raw.githubusercontent.com/Ebimsv/Machine_Learning_Course/refs/heads/main/pics/gradients_w_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate and Prepare Dataset\n",
    "We create a dataset with one feature (univariate), normalize it for faster convergence, and split it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset (univariate)\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=15, random_state=42)\n",
    "X = X.flatten()  # Flatten X to 1D array\n",
    "\n",
    "# Normalize X and y\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "y = (y - np.mean(y)) / np.std(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Parameters\n",
    "Randomly initialize the model parameters w (weight) and b (bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn()  # Random weight\n",
    "b = np.random.randn()  # Random bias\n",
    "learning_rate = 0.01  # Step size for updates\n",
    "epochs = 1000  # Number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Stochastic Gradient Descent\n",
    "We implement the SGD algorithm by iterating through the dataset, computing the gradients for w and b, and updating the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from sklearn.utils import shuffle as sklearn_shuffle  \n",
    "\n",
    "def sgd(X, y, w, b, learning_rate, epochs):      \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "We train the model using the training dataset and store the final parameters and loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model\n",
    "We evaluate the trained model on both the training and testing datasets by computing the Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize the Results\n",
    "We visualize the loss history and compare predicted vs actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
